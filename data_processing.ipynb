{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66f8a8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "282854e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install loguru >> stdout.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a75293e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.traceback import install; install()\n",
    "from pathlib import Path \n",
    "import shutil\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import tensorflow.keras.layers as layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import random\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a77db3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "\n",
    "drive_dir = Path('/content/drive/MyDrive')\n",
    "ucf_dir = drive_dir / \"UCF101/UCF-101\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5d2ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(drive_dir / f\"UCF101/train_paths.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5bdcbafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path = drive_dir / \"UCF101/logs\" \n",
    "log_path.mkdir(exist_ok=True)\n",
    "logger.remove()\n",
    "logger.add(str(log_path / \"first_frame_capture.log\"), level='DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85e49a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_frame(frame, output_shape):\n",
    "    # convert the values from uint8 to float 32 and also normalize values\n",
    "    # frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "\n",
    "    # pad the image with black while preserving the aspect ratio\n",
    "    # frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "\n",
    "    frame = frame.astype(np.float32) / 255.0\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "    target_h, target_w = output_shape\n",
    "    scale = min(target_h / h, target_w / w) # scale is selected so that the new image has a size that fits into target_h, target_w\n",
    "\n",
    "    new_h = int(h * scale)\n",
    "    new_w = int(w * scale)\n",
    "\n",
    "    # resize the image\n",
    "    resized = cv2.resize(frame, (new_w, new_h)) # the resize dim is width x height\n",
    "\n",
    "    # compute the padding\n",
    "    pad_h = (target_h - new_h) // 2\n",
    "    pad_w = (target_w - new_w) // 2\n",
    "\n",
    "    new_frame = np.zeros((target_h, target_w, 3))\n",
    "    new_frame[pad_h:pad_h + new_h, pad_w:pad_w + new_w] = resized\n",
    "    \n",
    "    return new_frame\n",
    "\n",
    "def capture_frames(video_path, frame_step=4, n_frames=32, output_size=(224, 224), mode='train'):\n",
    "    video_path = str(video_path)\n",
    "    video = cv2.VideoCapture(video_path) # open the video\n",
    "\n",
    "    if not video.isOpened():\n",
    "        video.release()\n",
    "        logger.warning(f'Failed to open video for {video_path}')\n",
    "        return\n",
    "\n",
    "    total_frames = video.get(cv2.CAP_PROP_FRAME_COUNT) # get total frame count\n",
    "    total_frames = int(total_frames) # cv2 return a float\n",
    "    required_frames = 1 + (n_frames - 1) * frame_step\n",
    "    result = []\n",
    "\n",
    "    if required_frames > total_frames:\n",
    "        start = 0 # if the required frames is greater than total frames available then we start sampling from the first frame\n",
    "    else: \n",
    "        # otherwise we pick a random starting point\n",
    "        max_start = total_frames - required_frames\n",
    "\n",
    "        # during inference or validation the start is not random\n",
    "        if mode == 'train':\n",
    "            start = random.randint(0, max_start) # 0 and max_start are inclusive\n",
    "        else:\n",
    "            start = 0\n",
    "    \n",
    "    # move the pointer to start\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "\n",
    "    # capture the first frame\n",
    "    # ret is a boolean indicating if frame was captured successfully\n",
    "    # frame is a numpy array of image (height, width, 3 channels BGR)\n",
    "    ret, frame = video.read() \n",
    "    if ret:\n",
    "        result.append(format_frame(frame, output_size))\n",
    "        logger.info(f'First frame captured from {video_path}')\n",
    "    else:\n",
    "        result.append()\n",
    "        logger.warning(f'Failed to capture first frame from {video_path}')\n",
    "        return \n",
    "    \n",
    "    # now start capturing remaining frames\n",
    "    for _ in range(n_frames - 1):\n",
    "        for _ in range(frame_step):\n",
    "            # skip frame_step number of frames\n",
    "            ret, frame = video.read()\n",
    "        if ret: # store the last frame in since skipping frame_step\n",
    "            result.append(format_frame(frame, output_size))\n",
    "        else:\n",
    "            result.append(np.zeros_like(result[0]))\n",
    "    \n",
    "    video.release()\n",
    "    # this [..., [2, 1, 0]] reads as all dimensions before the last one\n",
    "    # [2, 1, 0] change channel order from BGR to RGB\n",
    "    result_arr = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "    return result_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98783846",
   "metadata": {},
   "outputs": [],
   "source": [
    "jav_df = train_df.loc[train_df.action == \"JavelinThrow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f98cf681",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((jav_df.path.values, jav_df.class_idx.values))\n",
    "dataset = dataset.map(\n",
    "    lambda path, label: tf.py_function(\n",
    "        lambda p, l: (capture_frames(p.numpy().decode('utf-8')), l),\n",
    "        [path, label],\n",
    "        [tf.float32, tf.int64]\n",
    "    )\n",
    ")\n",
    "batch_size = 32\n",
    "dataset = dataset.batch(batch_size).shuffle(1000).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c6a8c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f9bbb345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_backbone():\n",
    "    input_layer = layers.Input(shape=(224, 224, 3))\n",
    "\n",
    "    x = input_layer # setting the input to a variable that changes is a common pattern\n",
    "    \n",
    "    # define the number of filters and where to add pooling\n",
    "    filters = [2**(5 + i) for i in range(5) for _ in range(2)]\n",
    "\n",
    "    for i, filter_size in enumerate(filters):\n",
    "        x = layers.Conv2D(filters=filter_size, kernel_size=(3, 3), padding='same')(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        if i % 2 != 0: # apply max pooling at 1, 3, 5, 7, 9 index; after 2nd layer of filters having the same size\n",
    "            x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    # finally to reduce the feature maps to flattened dimension or 1 x 1 \n",
    "    # we use globalaveragepooling 2d which will take average across the \n",
    "    # entire feature may and then the output will be number of channels\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=x) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9ca61156",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_backbone = build_cnn_backbone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3af40bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_lstm_model(cnn_backbone, num_classes=101):\n",
    "    input_layer = layers.Input(shape=(32, 224, 224, 3))\n",
    "    x = layers.TimeDistributed(cnn_backbone)(input_layer)\n",
    "\n",
    "    # apply the lstm\n",
    "    x = layers.LSTM(256, return_sequences=False)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(num_classes)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e036d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_cnn_lstm_model(cnn_backbone, num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f7e55c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e13145bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,720,160</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">787,456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,   │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m3\u001b[0m)                     │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m4,720,160\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m787,456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,540,641</span> (21.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,540,641\u001b[0m (21.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,536,673</span> (21.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,536,673\u001b[0m (21.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,968</span> (15.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,968\u001b[0m (15.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cbb1e7ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.fit(x[0], x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b54550f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
